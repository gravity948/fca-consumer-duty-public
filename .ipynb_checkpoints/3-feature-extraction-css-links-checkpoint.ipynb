{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the following information from every T&C html page\n",
    "1. Tokenized text from the main body (with src package)\n",
    "2. URL count from main body\n",
    "3. CSS style information for legibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment if you need to install any of the packages below\n",
    "# !python -m pip install --upgrade pip\n",
    "# !pip install langid\n",
    "# !pip install somajo\n",
    "# !pip install selenium\n",
    "# !pip install webdriver-manager\n",
    "# !pip install -U selenium --user\n",
    "# !pip install webdriver-manager\n",
    "# !pip install readability_lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from enum import Enum\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from src.StructuredLegalExtraction import extractTandC\n",
    "import src.ContentExtractor.ContentExtractorTypes\n",
    "from src.ContentExtractor import ContentExtractor\n",
    "from src.ContentExtractor.ContentExtractor import getMainContent\n",
    "from src.ContentExtractor.TreeUtilities import getFrequencyOfStyles\n",
    "import requests\n",
    "import readability_lxml\n",
    "from readability_lxml import Document\n",
    "import ast\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Firm</th>\n",
       "      <th>URL</th>\n",
       "      <th>Industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Whiteleys Financial Management LLP</td>\n",
       "      <td>http://www.whiteleysaccountants.co.uk/terms-an...</td>\n",
       "      <td>Investment Firms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Whiteleys Financial Management LLP</td>\n",
       "      <td>http://www.whiteleysaccountants.co.uk/privacy-...</td>\n",
       "      <td>Investment Firms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Whiteleys Financial Management LLP</td>\n",
       "      <td>http://www.whiteleysaccountants.co.uk/news-ite...</td>\n",
       "      <td>Investment Firms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Pioneer Point Partners LLP</td>\n",
       "      <td>https://pioneerpoint.com/privacy-and-cookie-po...</td>\n",
       "      <td>Investment Firms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Engage Wealth Management Limited</td>\n",
       "      <td>https://engagewm.co.uk/privacy-notice/</td>\n",
       "      <td>Investment Firms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4823</th>\n",
       "      <td>4837</td>\n",
       "      <td>Heddington Insurance (U.K.) Limited</td>\n",
       "      <td>https://www.chevron.com/privacy</td>\n",
       "      <td>Insurers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4824</th>\n",
       "      <td>4838</td>\n",
       "      <td>Heddington Insurance (U.K.) Limited</td>\n",
       "      <td>https://www.chevron.com/terms-of-use</td>\n",
       "      <td>Insurers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4825</th>\n",
       "      <td>4839</td>\n",
       "      <td>Lady Grover's Hospital Fund for Officers' Fami...</td>\n",
       "      <td>https://forceshealth.co.uk/terms-and-conditions/</td>\n",
       "      <td>Insurers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4826</th>\n",
       "      <td>4840</td>\n",
       "      <td>Lady Grover's Hospital Fund for Officers' Fami...</td>\n",
       "      <td>https://forceshealth.co.uk/wordpress/wp-conten...</td>\n",
       "      <td>Insurers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4827</th>\n",
       "      <td>4841</td>\n",
       "      <td>Lady Grover's Hospital Fund for Officers' Fami...</td>\n",
       "      <td>https://forceshealth.co.uk/cookie-policy/</td>\n",
       "      <td>Insurers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4828 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                               Firm  \\\n",
       "0              0                 Whiteleys Financial Management LLP   \n",
       "1              1                 Whiteleys Financial Management LLP   \n",
       "2              2                 Whiteleys Financial Management LLP   \n",
       "3              4                         Pioneer Point Partners LLP   \n",
       "4              5                   Engage Wealth Management Limited   \n",
       "...          ...                                                ...   \n",
       "4823        4837                Heddington Insurance (U.K.) Limited   \n",
       "4824        4838                Heddington Insurance (U.K.) Limited   \n",
       "4825        4839  Lady Grover's Hospital Fund for Officers' Fami...   \n",
       "4826        4840  Lady Grover's Hospital Fund for Officers' Fami...   \n",
       "4827        4841  Lady Grover's Hospital Fund for Officers' Fami...   \n",
       "\n",
       "                                                    URL          Industry  \n",
       "0     http://www.whiteleysaccountants.co.uk/terms-an...  Investment Firms  \n",
       "1     http://www.whiteleysaccountants.co.uk/privacy-...  Investment Firms  \n",
       "2     http://www.whiteleysaccountants.co.uk/news-ite...  Investment Firms  \n",
       "3     https://pioneerpoint.com/privacy-and-cookie-po...  Investment Firms  \n",
       "4                https://engagewm.co.uk/privacy-notice/  Investment Firms  \n",
       "...                                                 ...               ...  \n",
       "4823                    https://www.chevron.com/privacy          Insurers  \n",
       "4824               https://www.chevron.com/terms-of-use          Insurers  \n",
       "4825   https://forceshealth.co.uk/terms-and-conditions/          Insurers  \n",
       "4826  https://forceshealth.co.uk/wordpress/wp-conten...          Insurers  \n",
       "4827          https://forceshealth.co.uk/cookie-policy/          Insurers  \n",
       "\n",
       "[4828 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# links_df = pd.read_excel('data/URLs_Final.xlsx\n",
    "links_df = pd.read_excel('data/final_urls_all.xlsx')\n",
    "links_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting batch of links\n",
    "\n",
    "1st trial\n",
    "21/11/2022 19:35 - [:50] \n",
    "\n",
    "21/11/2022 20:08 - [50:100]\n",
    "\n",
    "21/11/2022 20:43 - [101:500]\n",
    "\n",
    "21/11/2022 22:28 - [501:1000]\n",
    "\n",
    "21/11/2022 23:32 - [1001:2000]\n",
    "\n",
    "22/11/2022 11:05 - [2001:2561]\n",
    "\n",
    "\n",
    "2nd trial\n",
    "\n",
    "22/11\n",
    "\n",
    "14:15 [:10]\n",
    "\n",
    "14:22 [11:50]\n",
    "\n",
    "14:37 [51:100]\n",
    "\n",
    "19:38 [101:1000]\n",
    "\n",
    "22:47 [1001:1400]\n",
    "\n",
    "23/11\n",
    "\n",
    "12:26 [1401:2000]\n",
    "\n",
    "19:34 [1873:2570]\n",
    "\n",
    "21:56 [2522:3500] (with final_urls_all.xlsx)\n",
    "\n",
    "24/11\n",
    "\n",
    "11:14 [3501:4200]\n",
    "\n",
    "16:24 [4201:4500]\n",
    "\n",
    "19:07 [4501:4827]\n",
    "\n",
    "3rd trial.....\n",
    "\n",
    "25/11 [0:500]\n",
    "\n",
    "2823"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = links_df.iloc[10:15]['URL']\n",
    "firms = links_df.iloc[10:15]['Firm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_df.iloc[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get Tokenized Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "def extractTandD_multiple(links, contentExtractor=src.ContentExtractor.ContentExtractorTypes.ContentExtractor.NaiveStyleAndShortTextExclusion,\n",
    "                          threshold=0.85, driver=None):\n",
    "    close = False\n",
    "    if driver is None:\n",
    "        close = True\n",
    "        driver = webdriver.Chrome(executable_path='../chromedriver')\n",
    "    results = []\n",
    "    links_count = 0\n",
    "    for link in links:\n",
    "        try:\n",
    "            results.append(extractTandC(link, contentExtractor, threshold, driver))\n",
    "        except:\n",
    "            results.append(np.NaN)\n",
    "        links_count += 1\n",
    "        clear_output(wait=True)\n",
    "        print(f'Progress: {links_count/len(links)*100}%')\n",
    "    if close:\n",
    "        driver.close()\n",
    "    return results\n",
    "\n",
    "token_text = extractTandD_multiple(links, driver = driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# character_counter = 0\n",
    "# text_extracted = []\n",
    "# content = json.loads(token_text[i])['content']\n",
    "\n",
    "# for j in np.arange(len(content)):\n",
    "#     character_counter += len(str(content[j]['subsections']))\n",
    "#     if character_counter >= 32500:\n",
    "#         character_counter_new = character_counter - len(str(content[j]['text']))\n",
    "#         text_extracted_new = []\n",
    "#         content_new = content[j]['subsections']\n",
    "\n",
    "#         for k in np.arange(len(content_new[j]['subsections'])):         \n",
    "#             character_counter_new += len(str(content[k]['subsections']))\n",
    "#             if character_counter_new >= 32500:\n",
    "#                 text_extracted.append(text_extracted_new)\n",
    "#                 break\n",
    "#             text_extracted_new.append(content_new[k]['subsections'])\n",
    "\n",
    "#             character_counter_new += len(str(content_new[k]['text']))\n",
    "#             if character_counter_new >= 32500:\n",
    "#                 text_extracted.append(text_extracted_new)\n",
    "#                 break\n",
    "#             text_extracted_new.append(content_new[k]['text'])\n",
    "\n",
    "#     text_extracted.append(content[j]['subsections'])\n",
    "\n",
    "#     character_counter += len(str(content[j]['text']))\n",
    "#     if character_counter >= 32500:\n",
    "#         break\n",
    "#     text_extracted.append(content[j]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_text_extracted = []\n",
    "\n",
    "for i in np.arange(len(token_text)):\n",
    "\n",
    "    character_counter = 0\n",
    "    text_extracted = []\n",
    "    \n",
    "    try:\n",
    "        content = json.loads(token_text[i])['content']\n",
    "        for j in np.arange(len(content)):\n",
    "            character_counter += len(str(content[j]['subsections']))\n",
    "            if character_counter >= 32500:\n",
    "                character_counter_new = character_counter - len(str(content[j]['text']))\n",
    "                text_extracted_new = []\n",
    "                content_new = content[j]['subsections']\n",
    "\n",
    "                for k in np.arange(len(content_new[j]['subsections'])):         \n",
    "                    character_counter_new += len(str(content[k]['subsections']))\n",
    "                    if character_counter_new >= 32500:\n",
    "                        text_extracted.append(text_extracted_new)\n",
    "                        break\n",
    "                    text_extracted_new.append(content_new[k]['subsections'])\n",
    "\n",
    "                    character_counter_new += len(str(content_new[k]['text']))\n",
    "                    if character_counter_new >= 32500:\n",
    "                        text_extracted.append(text_extracted_new)\n",
    "                        break\n",
    "                    text_extracted_new.append(content_new[k]['text'])\n",
    "\n",
    "            text_extracted.append(content[j]['subsections'])\n",
    "\n",
    "            character_counter += len(str(content[j]['text']))\n",
    "            if character_counter >= 32500:\n",
    "                break\n",
    "            text_extracted.append(content[j]['text'])\n",
    "            \n",
    "        token_text_extracted.append(text_extracted)\n",
    "        \n",
    "    except:\n",
    "        token_text_extracted.append(np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(str(token_text_extracted[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_text_extracted = []\n",
    "\n",
    "# for i in np.arange(len(token_text)):\n",
    "#     try:\n",
    "#         content = json.loads(token_text[i])['content']\n",
    "\n",
    "#         text_extracted = []\n",
    "\n",
    "#         for j in np.arange(len(content)):\n",
    "#             text_extracted.append(json.loads(token_text[i])['content'][j]['subsections'])\n",
    "#             text_extracted.append(json.loads(token_text[i])['content'][j]['text'])\n",
    "            \n",
    "#         token_text_extracted.append(text_extracted)\n",
    "            \n",
    "#     except:\n",
    "#         token_text_extracted.append(np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_text_extracted = []\n",
    "\n",
    "# for i in np.arange(len(token_text)):\n",
    "    \n",
    "    \n",
    "#     try:\n",
    "#         content = json.loads(token_text[i])['content']\n",
    "\n",
    "#         text_extracted = []\n",
    "\n",
    "#         for j in np.arange(len(content)):\n",
    "#             text_extracted.append(json.loads(token_text[i])['content'][j]['subsections'])\n",
    "#             text_extracted.append(json.loads(token_text[i])['content'][j]['text'])\n",
    "            \n",
    "#         token_text_extracted.append(text_extracted)\n",
    "            \n",
    "#     except:\n",
    "#         token_text_extracted.append(np.NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. URL Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_links(url):\n",
    "    r = requests.get(url, stream=True)\n",
    "    doc = Document(r.text)\n",
    "    main_content = doc.summary()\n",
    "    count = main_content.count('href')\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_count = []\n",
    "links_count = 0\n",
    "for link in links:\n",
    "    try:\n",
    "        url_count.append(count_links(link))\n",
    "    except:\n",
    "        url_count.append(np.NaN)\n",
    "    links_count += 1\n",
    "    clear_output(wait=True)\n",
    "    print(f'Progress: {links_count/len(links)*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_count = [None] * len(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. CSS Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# css_style = [None] * len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup as bs\n",
    "# from urllib.parse import urljoin\n",
    "\n",
    "# # URL of the web page you want to extract\n",
    "# url = \"https://www.hsbc.com/terms-and-conditions\"\n",
    "\n",
    "# # initialize a session\n",
    "# session = requests.Session()\n",
    "\n",
    "# # set the User-agent as a regular browser\n",
    "# session.headers[\"User-Agent\"] = \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36\"\n",
    "\n",
    "# # get the HTML content\n",
    "# html = session.get(url).content\n",
    "\n",
    "# # parse HTML using beautiful soup\n",
    "# soup = bs(html, \"html.parser\")\n",
    "# #print(soup.prettify())\n",
    "\n",
    "# # get the CSS files\n",
    "# css_files = []\n",
    "\n",
    "# for css in soup.find_all(\"link\", rel=\"stylesheet\"):\n",
    "#     if css.attrs.get(\"href\"):\n",
    "#         # if the link tag has the 'href' attribute\n",
    "#         css_url = urljoin(url, css.attrs.get(\"href\"))\n",
    "#         css_files.append(css_url)\n",
    "\n",
    "# css_files\n",
    "\n",
    "# with open(\"css_files.txt\", \"w\") as f:\n",
    "#     for css_file in css_files:\n",
    "#         print(css_file, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = links_df['URL']\n",
    "firms = links_df['Firm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       http://www.whiteleysaccountants.co.uk/terms-an...\n",
       "1       http://www.whiteleysaccountants.co.uk/privacy-...\n",
       "2       http://www.whiteleysaccountants.co.uk/news-ite...\n",
       "3       https://pioneerpoint.com/privacy-and-cookie-po...\n",
       "4                  https://engagewm.co.uk/privacy-notice/\n",
       "                              ...                        \n",
       "4823                      https://www.chevron.com/privacy\n",
       "4824                 https://www.chevron.com/terms-of-use\n",
       "4825     https://forceshealth.co.uk/terms-and-conditions/\n",
       "4826    https://forceshealth.co.uk/wordpress/wp-conten...\n",
       "4827            https://forceshealth.co.uk/cookie-policy/\n",
       "Name: URL, Length: 4828, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16min 46s, sys: 22.8 s, total: 17min 9s\n",
      "Wall time: 1h 5min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "css_stylesheets = []\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers[\"User-Agent\"] = \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36\"\n",
    "for i in links:\n",
    "    try:\n",
    "        html = session.get(i).content\n",
    "        soup = bs(html, \"html.parser\")\n",
    "        css_files_empty = []\n",
    "        for css in soup.find_all(\"link\", rel=\"stylesheet\"):\n",
    "            if css.attrs.get(\"href\"):\n",
    "                # if the link tag has the 'href' attribute\n",
    "                css_url = urljoin(i, css.attrs.get(\"href\"))\n",
    "                css_files_empty.append(css_url)\n",
    "        css_stylesheets.append(css_files_empty)\n",
    "    except:\n",
    "        css_stylesheets.append(np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4828"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(css_stylesheets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(pd.isnull(css_stylesheets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Firm': firms,\n",
    "             'CSS Stylesheets': css_stylesheets}).to_csv('data/css_stylesheets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to one spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame({'Firm': firms,\n",
    "                        'Link': links,\n",
    "                        'Tokenized Text': token_text_extracted, \n",
    "                        'URL Count': url_count, \n",
    "                        'CSS Style': css_style})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df.to_excel(\"data/data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pd.ExcelWriter('./data/data_trial.xlsx', mode='w') as writer:\n",
    "#     data_df.to_excel(writer, sheet_name='data', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('./data/data_trial.xlsx', engine='openpyxl', mode='a', if_sheet_exists=\"overlay\") as writer:\n",
    "    data_df.to_excel(writer, sheet_name='data', startrow=writer.sheets['data'].max_row, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.Series(token_text_extracted).isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
